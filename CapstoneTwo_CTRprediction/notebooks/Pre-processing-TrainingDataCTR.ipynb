{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d071da51-fc6f-4155-8ac8-27e3c9709ac2",
   "metadata": {},
   "source": [
    "This notebook prepares the CTR prediction dataset for machine learning model development by applying standard preprocessing steps. \n",
    "\n",
    "The goal is to transform the raw, cleaned dataset into a well-structured, numeric format suitable for model training.\n",
    "\n",
    "Create a cleaned development dataset by:\n",
    "- Encoding categorical features\n",
    "- Standardizing numerical features\n",
    "- Splitting data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "944321b8-e8f2-459d-9a57-54c54c9f318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from category_encoders) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from category_encoders) (2.2.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.6)\n",
      "Collecting scikit-learn>=1.6.0 (from category_encoders)\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from category_encoders) (1.13.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from category_encoders) (0.14.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from scikit-learn>=1.6.0->category_encoders) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from scikit-learn>=1.6.0->category_encoders) (3.5.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\vidus\\anaconda3\\lib\\site-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
      "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.4/10.7 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 18.5 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-learn, category_encoders\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.1\n",
      "    Uninstalling scikit-learn-1.5.1:\n",
      "      Successfully uninstalled scikit-learn-1.5.1\n",
      "Successfully installed category_encoders-2.8.1 scikit-learn-1.7.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install category_encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f44976-14b4-4464-9e65-5a967bbc630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CTR-Pre-processingAndTrainingDataDevelopment\n",
    "#loading the necessary packages\n",
    "\n",
    "#%reset\n",
    "%reset_selective -f regex\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b65da61-f4a4-4ea3-835e-1dddf840981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75058 entries, 0 to 75057\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   banner_pos        75058 non-null  int64\n",
      " 1   device_type       75058 non-null  int64\n",
      " 2   device_conn_type  75058 non-null  int64\n",
      " 3   site_id           75058 non-null  int64\n",
      " 4   app_id            75058 non-null  int64\n",
      " 5   device_model      75058 non-null  int64\n",
      " 6   C14               75058 non-null  int64\n",
      " 7   C15               75058 non-null  int64\n",
      " 8   C16               75058 non-null  int64\n",
      " 9   C17               75058 non-null  int64\n",
      " 10  C18               75058 non-null  int64\n",
      " 11  C19               75058 non-null  int64\n",
      " 12  C20               75058 non-null  int64\n",
      " 13  C21               75058 non-null  int64\n",
      " 14  click             75058 non-null  int64\n",
      "dtypes: int64(15)\n",
      "memory usage: 8.6 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load cleaned data\n",
    "file_path = \"/mnt/data/cleaned_ctr_prediction_data.csv\"\n",
    "ctr_df = pd.read_csv(r\"C:\\Users\\vidus\\Projects\\Springboard\\CapstoneTwo_CTRprediction\\data\\processed\\cleaned_ctr_prediction_data.csv\")\n",
    "\n",
    "# Identify categorical and numeric features\n",
    "categorical_low_card = ['banner_pos', 'device_type', 'device_conn_type']\n",
    "categorical_high_card = ['site_id', 'app_id', 'device_model']\n",
    "numeric_features = ['C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21']\n",
    "target_variable = 'click'\n",
    "\n",
    "# Preview data to confirm structure\n",
    "ctr_df[categorical_low_card + categorical_high_card + numeric_features + [target_variable]].info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60829e73-d07c-4122-a6ac-0a34127f580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create dummy variables for low-cardinality categorical features\n",
    "categorical_low_card = ['banner_pos', 'device_type', 'device_conn_type']\n",
    "ctr_df_dummies = pd.get_dummies(ctr_df, columns=categorical_low_card, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760750b-154f-490d-99ba-afd5b2112849",
   "metadata": {},
   "source": [
    "Above I have created Dummy Variables for Low-Cardinality Categorical Features\n",
    "Low-cardinality categorical variables (e.g., `banner_pos`, `device_type`) were one-hot encoded using `pd.get_dummies()` to convert them into binary features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50acec00-7064-4334-9ab1-9b313927f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "ctr_df_dummies[numeric_features] = scaler.fit_transform(ctr_df_dummies[numeric_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82f878-fac0-4afc-8aee-9d612906d09a",
   "metadata": {},
   "source": [
    "In the above Step 2, Target Encode High-Cardinality Categorical Features\n",
    "\n",
    "High-cardinality features like:\n",
    "\n",
    "- site_id\n",
    "\n",
    "- app_id\n",
    "\n",
    "- device_model\n",
    "\n",
    "can introduce sparsity and noise when one-hot encoded.\n",
    "\n",
    "Instead, I apply Target Encoding, which replaces each category with the mean of the target variable (click) within that category. \n",
    "\n",
    "This reduces dimensionality while retaining predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f71a50e-7932-4335-b93c-248dedfec54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Target Encode High-Cardinality Features\n",
    "# Target encoding helps with features like site_id, app_id, device_model.\n",
    "#Using category_encoders.TargetEncoder, we replace each category with the mean of the target (click) for that category.\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Instantiate target encoder\n",
    "target_encoder = ce.TargetEncoder(cols=['site_id', 'app_id', 'device_model'])\n",
    "\n",
    "# Fit and transform\n",
    "ctr_df_encoded = target_encoder.fit_transform(ctr_df_dummies, ctr_df['click'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf1553-9ba7-4627-9cdf-9121be01e249",
   "metadata": {},
   "source": [
    "In the above Step 3: Target Encode High-Cardinality Features\n",
    "\n",
    "Some categorical variables in this dataset, such as `site_id`, `app_id`, and `device_model`, have a large number of unique categories (i.e., high cardinality). Applying one-hot encoding to these would create a very sparse dataset and potentially introduce noise or overfitting in models.\n",
    "\n",
    "To address this, I use **Target Encoding** via `category_encoders.TargetEncoder`, which replaces each category with the **mean of the target variable (`click`)** for that category. This approach reduces dimensionality while still preserving useful predictive information.\n",
    "\n",
    "This step is crucial for improving model performance when dealing with high-cardinality categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d5d9e9f-a8be-4d82-87a9-f553dc23a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4:  Standard Scale the Numeric Features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_features = ['C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21']\n",
    "scaler = StandardScaler()\n",
    "ctr_df_encoded[numeric_features] = scaler.fit_transform(ctr_df_encoded[numeric_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddef329-e1a7-4fd7-b80f-fee738cbb2f8",
   "metadata": {},
   "source": [
    "In the above Step 4: Standardize the Numeric Features\n",
    "\n",
    "To ensure that all numeric features contribute equally to the model and to improve optimization convergence for algorithms like Logistic Regression or Gradient Boosting, we apply **standard scaling**.\n",
    "\n",
    "We use `StandardScaler` from `sklearn.preprocessing` to transform each numeric feature (`C14` to `C21`) such that they have a **mean of 0 and standard deviation of 1**. This prevents features with larger magnitudes from dominating the model training process.\n",
    "\n",
    "Only numeric features are standardized—categorical features (including dummy or encoded ones) are not scaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62bfd0a6-730c-42d5-9202-f4857c06325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (60046, 31)\n",
      "Test set shape: (15012, 31)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Split into Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X and y\n",
    "X = ctr_df_encoded.drop(columns=['click'])\n",
    "y = ctr_df_encoded['click']\n",
    "\n",
    "# Train-Test Split with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Optional: Check shape\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815a1d5-8e8d-4cf3-80eb-a25c62c42d36",
   "metadata": {},
   "source": [
    "Step 5: Split into Training and Testing Sets\n",
    "To evaluate model performance fairly and avoid data leakage, we split our dataset into training and testing subsets:\n",
    "\n",
    "Training Set: Used to train the machine learning model.\n",
    "\n",
    "Testing Set: Used to evaluate the model’s performance on unseen data.\n",
    "\n",
    "We use the train_test_split() function from sklearn.model_selection with the following configurations:\n",
    "\n",
    "test_size=0.2: 20% of the data is reserved for testing.\n",
    "\n",
    "random_state=42: Ensures reproducibility of the split.\n",
    "\n",
    "stratify=y: Maintains the original class distribution (important for imbalanced data like CTR).\n",
    "\n",
    "This ensures the model is trained on a representative subset of the data and evaluated on an equally balanced holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b545ccd-7a7a-4ea5-90d3-7d8c4a4f7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use raw string (prefix with 'r') to avoid issues with backslashes\n",
    "## Save datasets in the processed data folder\n",
    "X_train.to_csv(r\"C:\\Users\\vidus\\Projects\\Springboard\\CapstoneTwo_CTRprediction\\data\\processed\\X_train.csv\", index=False)\n",
    "X_test.to_csv(r\"C:\\Users\\vidus\\Projects\\Springboard\\CapstoneTwo_CTRprediction\\data\\processed\\X_test.csv\", index=False)\n",
    "y_train.to_csv(r\"C:\\Users\\vidus\\Projects\\Springboard\\CapstoneTwo_CTRprediction\\data\\processed\\y_train.csv\", index=False)\n",
    "y_test.to_csv(r\"C:\\Users\\vidus\\Projects\\Springboard\\CapstoneTwo_CTRprediction\\data\\processed\\y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2f2c6-3405-4b55-b43d-333451eb108b",
   "metadata": {},
   "source": [
    "Save Processed Datasets\n",
    "We save the train and test sets as .csv files for reuse in the modeling phase:\n",
    "\n",
    "X_train.csv, X_test.csv\n",
    "\n",
    "y_train.csv, y_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd524c-ef19-443f-88d4-cf36afd579de",
   "metadata": {},
   "source": [
    "Next Step: \n",
    "Use the preprocessed datasets to train and evaluate machine learning models during the modeling phase of the capstone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6801345-102e-45bf-9af3-4e75ffd95e37",
   "metadata": {},
   "source": [
    "### Feature Type Justification\n",
    "\n",
    "We identified categorical and continuous features through data types and domain understanding.  \n",
    "- **Categorical Features**: `'banner_pos'`, `'device_type'`, and `'device_conn_type'` have a small number of unique values and are encoded using one-hot encoding.  \n",
    "- **High Cardinality Categorical**: `'site_id'`, `'app_id'`, and `'device_model'` have a large number of unique categories, so we applied target encoding to avoid high dimensionality.  \n",
    "- **Continuous Features**: `'C14'` to `'C21'` are treated as numeric based on their value distribution and usage in previous CTR prediction literature.  \n",
    "We applied StandardScaler only on these numeric features to standardize their magnitude, which is important for many machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85d7a2-c18b-4a6d-b9e1-13020add3fad",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "In this notebook, I successfully completed the Pre-processing and Training Data Development phase for the Click-Through Rate (CTR) prediction project. Key steps included:\n",
    "\n",
    "Dummy Encoding: Applied one-hot encoding to low-cardinality categorical features (banner_pos, device_type, device_conn_type) to prepare them for modeling.\n",
    "\n",
    "Target Encoding: Handled high-cardinality categorical features (site_id, app_id, device_model) using target encoding, which reduces dimensionality while preserving meaningful patterns with respect to the target variable click.\n",
    "\n",
    "Standardization: Scaled continuous numeric features (C14 to C21) using StandardScaler to ensure features contribute equally to model learning.\n",
    "\n",
    "Train-Test Split: Split the dataset into training and testing subsets using an 80/20 ratio while maintaining the target distribution (stratify=y) to ensure fair model evaluation.\n",
    "\n",
    "Data Export: Saved the final training and testing datasets as CSV files for use in the next modeling step.\n",
    "\n",
    "With these preprocessing steps complete, now have a clean and standardized dataset ready for building and evaluating predictive machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71ee19-64dc-4048-bcf4-816b71042fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
